{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdb9fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Incrementally build a centroid table.\n",
    "\n",
    "For every source file named  grid_<zone>.parquet  in <input_dir>\n",
    "  • compute WGS-84 centroids,\n",
    "  • append them as ONE row-group (one part file) to <output_dataset>,\n",
    "  • remember progress in <output_dataset>.done  so the job can restart.\n",
    "\n",
    "Output:\n",
    "  <output_dataset>/part.<n>.parquet   ← one per processed zone\n",
    "  <output_dataset>.done               ← JSON list of processed filenames\n",
    "Readable by: fastparquet, pyarrow, pandas, dask.\n",
    "Requires: fastparquet ≥ 0.7, geopandas, pyarrow.\n",
    "\"\"\"\n",
    "\n",
    "import json, os, re, gc, sys\n",
    "from pathlib import Path\n",
    "\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from fastparquet import write\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# helpers\n",
    "# ---------------------------------------------------------------------------\n",
    "def extract_zone_order(tile_id: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Produces a sortable key so final output is NP → UTM(1N…60S) → SP.\n",
    "    \"\"\"\n",
    "    def _key(tid: str) -> int:\n",
    "        m = re.search(r\"G\\d+m_(\\w+)_\", tid)\n",
    "        if not m:\n",
    "            return 9999\n",
    "        zone = m.group(1)\n",
    "        if zone == \"NP\":\n",
    "            return -1\n",
    "        if zone == \"SP\":\n",
    "            return 9998\n",
    "        m2 = re.match(r\"(\\d+)([NS])\", zone)\n",
    "        if not m2:\n",
    "            return 9999\n",
    "        znum, hemi = int(m2[1]), m2[2]\n",
    "        return znum * 2 + (0 if hemi == \"N\" else 1)\n",
    "\n",
    "    return tile_id.apply(_key)\n",
    "\n",
    "\n",
    "def append_df(df: pd.DataFrame, dataset_path: Path) -> None:\n",
    "    \"\"\"\n",
    "    Append `df` as ONE new row-group (one new part file) to <dataset_path>.\n",
    "    \"\"\"\n",
    "    write(\n",
    "        dataset_path,               # a directory will be created on 1st call\n",
    "        df,\n",
    "        compression=\"ZSTD\",\n",
    "        append=dataset_path.exists(),   # True after first zone\n",
    "        file_scheme=\"simple\",          # 1 part file per append, restart-safe\n",
    "        object_encoding=\"utf8\",        # geometry stored as WKT\n",
    "    )\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# main driver\n",
    "# ---------------------------------------------------------------------------\n",
    "def combine_grid_parquets_centroid_incremental(\n",
    "    input_dir: str | Path,\n",
    "    output_dataset: str | Path,\n",
    "    checkpoint_file: str | Path | None = None,\n",
    ") -> None:\n",
    "    input_dir      = Path(input_dir)\n",
    "    output_dataset = Path(output_dataset)\n",
    "    checkpoint     = Path(checkpoint_file) if checkpoint_file else output_dataset.with_suffix(\".done\")\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "    # 1. Already processed zones\n",
    "    done: set[str] = set()\n",
    "    if checkpoint.exists():\n",
    "        done.update(json.loads(checkpoint.read_text()))\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "    # 2. Iterate over sources\n",
    "    sources = sorted(\n",
    "        p for p in input_dir.iterdir()\n",
    "        if p.name.startswith(\"grid_\") and p.suffix == \".parquet\"\n",
    "    )\n",
    "\n",
    "    for src in tqdm(sources, desc=\"UTM zones\"):\n",
    "        if src.name in done:\n",
    "            continue          # skip if already finished earlier\n",
    "\n",
    "        try:\n",
    "            # -- read minimal columns\n",
    "            gdf = gpd.read_parquet(src, columns=[\"tile_id\", \"geometry\"])\n",
    "            if gdf.crs is None:\n",
    "                raise ValueError(\"missing CRS\")\n",
    "\n",
    "            # -- accurate centroid in equal-area, then back to WGS-84\n",
    "            gdf_proj  = gdf.to_crs(\"EPSG:6933\")\n",
    "            centroids = (\n",
    "                gdf_proj.geometry.centroid\n",
    "                .to_crs(\"EPSG:4326\")\n",
    "                .apply(lambda geom: geom.wkt)\n",
    "            )\n",
    "\n",
    "            chunk = pd.DataFrame(\n",
    "                {\n",
    "                    \"tile_id\": gdf[\"tile_id\"].values,\n",
    "                    \"geometry\": centroids.values,\n",
    "                    \"zone_order\": extract_zone_order(gdf[\"tile_id\"]),\n",
    "                }\n",
    "            ).sort_values(\"zone_order\").drop(columns=\"zone_order\")\n",
    "\n",
    "            append_df(chunk, output_dataset)\n",
    "\n",
    "            # -- mark success\n",
    "            done.add(src.name)\n",
    "            checkpoint.write_text(json.dumps(sorted(done)))\n",
    "\n",
    "            # -- memory hygiene\n",
    "            del gdf, gdf_proj, centroids, chunk\n",
    "            gc.collect()\n",
    "\n",
    "        except Exception as exc:\n",
    "            print(f\"[WARN] {src.name} skipped: {exc}\", file=sys.stderr)\n",
    "\n",
    "    print(f\"✅ Finished. Dataset at {output_dataset} now contains {len(done)} zones.\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "# CLI wrapper (edit the three paths then run)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87dc4751",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to current sandbox directory\n",
    "input_directory = \"D:/nesteo_hf/grids/grid_600m\"\n",
    "output_file = \"D:/nesteo_hf/grids/grids_centers/grid_600m.parquet\"\n",
    "combine_grid_parquets_centroid_incremental(input_directory, output_file)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
