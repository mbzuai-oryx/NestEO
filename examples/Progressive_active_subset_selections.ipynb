{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4acb6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exapnding landcover proportions from a parquet file to individual columns\n",
    "\n",
    "import dask.dataframe as dd\n",
    "import ast\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from dask.diagnostics import ProgressBar\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# -----------------------------------------------------------------------------------\n",
    "# Landcover Setup\n",
    "# -----------------------------------------------------------------------------------\n",
    "landcover_legend = {\n",
    "    0: \"Open Seas\", 10: \"Tree Cover\", 20: \"Shrubland\", 30: \"Grassland\",\n",
    "    40: \"Cropland\", 50: \"Built-up\", 60: \"Bare/Sparse Vegetation\", 70: \"Snow and Ice\",\n",
    "    80: \"Permanent Water Bodies\", 90: \"Herbaceous Wetland\", 95: \"Mangroves\", 100: \"Moss and Lichen\"\n",
    "}\n",
    "lc_columns = list(landcover_legend.values())\n",
    "\n",
    "# -----------------------------------------------------------------------------------\n",
    "# Expansion Function\n",
    "# -----------------------------------------------------------------------------------\n",
    "def expand_landcover_props_fast(df_partition):\n",
    "    parsed = df_partition['landcover_props'].apply(ast.literal_eval)\n",
    "\n",
    "    for col in lc_columns:\n",
    "        df_partition[col] = 0.0\n",
    "\n",
    "    for code, col_name in landcover_legend.items():\n",
    "        df_partition[col_name] = parsed.apply(lambda d: d.get(code, 0.0))\n",
    "\n",
    "    df_partition = df_partition.drop(columns=[\"landcover_props\"])\n",
    "\n",
    "    final_columns = [\"tile_id\"] + lc_columns\n",
    "    df_partition = df_partition[final_columns]\n",
    "\n",
    "    return df_partition\n",
    "\n",
    "# -----------------------------------------------------------------------------------\n",
    "# Paths\n",
    "# -----------------------------------------------------------------------------------\n",
    "input_folder_pattern = \"D:/NestEO_test_outputs/grid_1200m/lc_proportions_10m_*_1200m.parquet\"\n",
    "output_folder = \"D:/NestEO_test_outputs/grid_1200m/lc_props_with_columns/\"\n",
    "\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# -----------------------------------------------------------------------------------\n",
    "# Process Each Zone File Separately\n",
    "# -----------------------------------------------------------------------------------\n",
    "zone_files = sorted(glob.glob(input_folder_pattern))\n",
    "\n",
    "print(f\"Found {len(zone_files)} zone files to process.\")\n",
    "\n",
    "for file_path in zone_files:\n",
    "    zone_name = os.path.basename(file_path).replace(\".parquet\", \"\")  # e.g., lc_proportions_10m_32N_1200m\n",
    "    output_file = os.path.join(output_folder, f\"{zone_name}.parquet\")\n",
    "\n",
    "    if os.path.exists(output_file):\n",
    "        print(f\"Skipping {zone_name}, already processed.\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\nProcessing {zone_name}...\")\n",
    "\n",
    "    # Read the zone file lazily\n",
    "    df = dd.read_parquet(\n",
    "        file_path,\n",
    "        engine=\"pyarrow\",\n",
    "        columns=[\"tile_id\", \"landcover_props\"]\n",
    "    )\n",
    "\n",
    "    # Optional: repartition to larger partitions if needed\n",
    "    df = df.repartition(partition_size=\"3GB\")\n",
    "\n",
    "    # Define correct meta\n",
    "    meta_columns = {\"tile_id\": 'object'}\n",
    "    meta_columns.update({col: 'float32' for col in lc_columns})\n",
    "\n",
    "    # Expand\n",
    "    df_expanded = df.map_partitions(expand_landcover_props_fast, meta=meta_columns)\n",
    "\n",
    "    # Compute and Save\n",
    "    with ProgressBar():\n",
    "        df_final = df_expanded.compute()\n",
    "\n",
    "    # Save cleanly\n",
    "    table = pa.Table.from_pandas(df_final, preserve_index=False)\n",
    "    pq.write_table(\n",
    "        table,\n",
    "        where=output_file,\n",
    "        compression=\"snappy\",\n",
    "        use_dictionary=True,\n",
    "        data_page_size=2097152,\n",
    "        row_group_size=1000000\n",
    "    )\n",
    "\n",
    "    print(f\"Saved expanded zone: {output_file}\")\n",
    "\n",
    "print(\"\\nAll zones processed and saved successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a6b3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering non-Open Seas Landcover Proportions for the files with expanded columns\n",
    "# -----------------------------------------------------------------------------------\n",
    "import glob\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import os\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------------\n",
    "# Landcover Setup\n",
    "# -----------------------------------------------------------------------------------\n",
    "landcover_legend = {\n",
    "    0: \"Open Seas\", 10: \"Tree Cover\", 20: \"Shrubland\", 30: \"Grassland\",\n",
    "    40: \"Cropland\", 50: \"Built-up\", 60: \"Bare/Sparse Vegetation\", 70: \"Snow and Ice\",\n",
    "    80: \"Permanent Water Bodies\", 90: \"Herbaceous Wetland\", 95: \"Mangroves\", 100: \"Moss and Lichen\"\n",
    "}\n",
    "landcover_columns = list(landcover_legend.values())\n",
    "\n",
    "# -----------------------------------------------------------------------------------\n",
    "# Input and output paths\n",
    "input_folder = \"D:/NestEO_test_outputs/grid_1200m/lc_props_with_columns/\"\n",
    "output_folder = \"D:/NestEO_test_outputs/grid_1200m/lc_props_with_columns_filtered/\"\n",
    "\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# List all parquet files\n",
    "files = sorted(glob.glob(os.path.join(input_folder, \"*.parquet\")))\n",
    "\n",
    "# Process each file separately\n",
    "for i, f in enumerate(files):\n",
    "    print(f\"Processing {i+1}/{len(files)}: {os.path.basename(f)}\")\n",
    "\n",
    "    # Read one file at a time\n",
    "    df = pd.read_parquet(f, engine=\"pyarrow\")\n",
    "\n",
    "    # Apply filter immediately\n",
    "    df_filtered = df[df['Open Seas'] < 0.999].copy()\n",
    "\n",
    "    # Convert all landcover columns to float32\n",
    "    df_filtered[landcover_columns] = df_filtered[landcover_columns].astype('float32')\n",
    "\n",
    "    \n",
    "    # Save immediately\n",
    "    output_path = os.path.join(output_folder, os.path.basename(f))\n",
    "    df_filtered.to_parquet(output_path, index=False, engine=\"pyarrow\", compression=\"snappy\")\n",
    "\n",
    "    # Clean memory manually\n",
    "    del df\n",
    "    del df_filtered\n",
    "\n",
    "print(\"\\nAll zone files filtered and saved separately.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56906a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering a few Open Seas Landcover Proportions to add to the previous non-Open Seas Landcover Proportions\n",
    "# -----------------------------------------------------------------------------------\n",
    "\n",
    "import glob\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import os\n",
    "import gc\n",
    "\n",
    "# -----------------------------------------------------------------------------------\n",
    "# Landcover Setup\n",
    "# -----------------------------------------------------------------------------------\n",
    "landcover_legend = {\n",
    "    0: \"Open Seas\", 10: \"Tree Cover\", 20: \"Shrubland\", 30: \"Grassland\",\n",
    "    40: \"Cropland\", 50: \"Built-up\", 60: \"Bare/Sparse Vegetation\", 70: \"Snow and Ice\",\n",
    "    80: \"Permanent Water Bodies\", 90: \"Herbaceous Wetland\", 95: \"Mangroves\", 100: \"Moss and Lichen\"\n",
    "}\n",
    "landcover_columns = list(landcover_legend.values())\n",
    "\n",
    "# -----------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# Input and output paths\n",
    "input_folder = \"D:/NestEO_test_outputs/grid_1200m/lc_props_with_columns/\"\n",
    "output_folder = \"D:/NestEO_test_outputs/grid_1200m/lc_props_with_columns_openseas_filtered/\"\n",
    "\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# List all parquet files\n",
    "files = sorted(glob.glob(os.path.join(input_folder, \"*.parquet\")))\n",
    "\n",
    "# Process each file separately\n",
    "for i, f in enumerate(files):\n",
    "    print(f\"Processing {i+1}/{len(files)}: {os.path.basename(f)}\")\n",
    "\n",
    "    # Read one file at a time\n",
    "    df = pd.read_parquet(f, engine=\"pyarrow\")\n",
    "\n",
    "    # Apply filter immediately\n",
    "    df_filtered = df[df['Open Seas'] > 0.999].copy()\n",
    "    del df\n",
    "    gc.collect()\n",
    "\n",
    "    # Convert all landcover columns to float32\n",
    "    df_filtered[landcover_columns] = df_filtered[landcover_columns].astype('float32')\n",
    "\n",
    "    # sample_factor = len(df_filtered) / 1000_000\n",
    "    df_filtered = df_filtered.sample(frac=0.01, random_state=42)\n",
    "\n",
    "    # Save immediately\n",
    "    output_path = os.path.join(output_folder, os.path.basename(f))\n",
    "    df_filtered.to_parquet(output_path, index=False, engine=\"pyarrow\", compression=\"snappy\")\n",
    "\n",
    "    # Clean memory manually\n",
    "    # del df\n",
    "    del df_filtered\n",
    "\n",
    "print(\"\\nAll zone files filtered and saved separately.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d6b856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merging all filtered Non- Open Seas Landcover Proportions to the previous non-Open Seas Landcover Proportions\n",
    "import glob\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "files = glob.glob(\"D:/NestEO_test_outputs/grid_1200m/lc_props_with_columns_filtered/*.parquet\")\n",
    "\n",
    "dfs = []\n",
    "for i, f in enumerate(files):\n",
    "    print(i+1, f)\n",
    "    df = pd.read_parquet(f)\n",
    "    df = df[df['Open Seas'] < 0.999].copy()\n",
    "    dfs.append(df)\n",
    "    del df\n",
    "\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "df.to_parquet(\"D:/NestEO_test_outputs/grid_1200m/lc_props_with_columns_filtered/lc_props_non_openseas_all_zones_test.parquet\", index=False, engine='pyarrow', compression='snappy')\n",
    "\n",
    "# Open Seas Landcover Proportions\n",
    "# -----------------------------------------------------------------------------------\n",
    "\n",
    "import glob\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "files = glob.glob(\"D:/NestEO_test_outputs/grid_1200m/lc_props_with_columns_openseas_filtered/*.parquet\")\n",
    "\n",
    "dfs = []\n",
    "for i, f in enumerate(files):\n",
    "    print(i+1, f)\n",
    "    df = pd.read_parquet(f)\n",
    "    df = df[df['Open Seas'] > 0.999].copy()\n",
    "    dfs.append(df)\n",
    "    del df\n",
    "\n",
    "df = pd.concat(dfs, ignore_index=True)\n",
    "##########################################\n",
    "df = df.sample(n=1050000, random_state=42)  # sample 1 million rows\n",
    "##########################################\n",
    "\n",
    "df.to_parquet(\"D:/NestEO_test_outputs/grid_1200m/lc_props_with_columns_openseas_filtered/lc_props_some_openseas_all_zones_test.parquet\", index=False, engine='pyarrow', compression='snappy')\n",
    "\n",
    "# Merging all filtered Non- Open Seas Landcover Proportions to the previous non-Open Seas Landcover Proportions\n",
    "# -----------------------------------------------------------------------------------\n",
    "df_non_openseas = pd.read_parquet(\"D:/NestEO_test_outputs/grid_1200m/lc_props_with_columns_filtered/lc_props_non_openseas_all_zones.parquet\")\n",
    "df_openseas = pd.read_parquet(\"D:/NestEO_test_outputs/grid_1200m/lc_props_with_columns_openseas_filtered/lc_props_some_openseas_all_zones.parquet\")\n",
    "\n",
    "df = pd.concat([df_non_openseas, df_openseas], ignore_index=True)\n",
    "df.to_parquet(\"D:/NestEO_test_outputs/grid_1200m/filtered_lc_props_all_zones.parquet\", index=False, engine='pyarrow', compression='snappy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3710403",
   "metadata": {},
   "source": [
    "\n",
    "# Now, below we start on subset selections from this filtered dataset\n",
    "## Above we filtered non-openseas 105 million cells, and added 1.05 million open seas cell for potential filtering later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b00d116",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"D:/NestEO_test_outputs/grid_1200m/filtered_lc_props_all_zones.parquet\")\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd261663",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860eacae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding major landcover classes to the filtered file\n",
    "# -----------------------------------------------------------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming df is your DataFrame\n",
    "landcover_columns = [\n",
    "    \"Open Seas\", \"Tree Cover\", \"Shrubland\", \"Grassland\", \"Cropland\", \"Built-up\",\n",
    "    \"Bare/Sparse Vegetation\", \"Snow and Ice\", \"Permanent Water Bodies\",\n",
    "    \"Herbaceous Wetland\", \"Mangroves\", \"Moss and Lichen\"\n",
    "]\n",
    "\n",
    "# Sum landcover fractions across all rows for each class\n",
    "landcover_totals = df[landcover_columns].sum()\n",
    "\n",
    "# Compute total landcover (all landcover values across all rows and classes)\n",
    "total_sum = landcover_totals.sum()\n",
    "\n",
    "# Compute global proportions\n",
    "global_proportions = landcover_totals / total_sum\n",
    "\n",
    "# Optional: convert to percentage\n",
    "global_proportions_percent = global_proportions * 100\n",
    "\n",
    "\n",
    "global_proportions_percent.sort_values(ascending=False).to_frame().reset_index().rename(columns={\"index\": \"Landcover\", 0: \"Proportion (%)\"}).to_csv(\"D:/NestEO_test_outputs/grid_1200m/lc_props_with_columns_filtered/global_landcover_proportions.csv\", index=False)\n",
    "global_proportions_percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e500814",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Majority_LC\"] = df[landcover_columns].idxmax(axis=1)\n",
    "global_majority_lc = df[\"Majority_LC\"].value_counts(normalize=True) * 100\n",
    "# global_majority_lc = global_majority_lc.sort_index()\n",
    "global_majority_lc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be1b3dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet(\"D:/NestEO_test_outputs/grid_1200m/filtered_lc_props_all_zones_107million.parquet\", index=False, engine='pyarrow', compression='snappy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc6fa77",
   "metadata": {},
   "source": [
    "## Reading 107million cells of 1200 for later subset selections\n",
    "###\n",
    "###\n",
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2b7e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding Shannon Entropy to the filtered file\n",
    "# -----------------------------------------------------------------------------------\n",
    "import pandas as pd\n",
    "df = pd.read_parquet(\"D:/NestEO_test_outputs/grid_1200m/filtered_lc_props_all_zones_107million.parquet\")\n",
    "from scipy.stats import entropy\n",
    "import numpy as np, pandas as pd\n",
    "rng = np.random.default_rng(42)                               # reproducible\n",
    "\n",
    "def compute_shannon_entropy(row):\n",
    "    p = np.asarray(row, dtype=\"float32\")\n",
    "    p = p[p > 0]\n",
    "    return entropy(p, base=2)\n",
    "\n",
    "landcover_cols = [\n",
    "    \"Open Seas\", \"Tree Cover\", \"Shrubland\", \"Grassland\", \"Cropland\",\n",
    "    \"Built-up\", \"Bare/Sparse Vegetation\", \"Snow and Ice\",\n",
    "    \"Permanent Water Bodies\", \"Herbaceous Wetland\",\n",
    "    \"Mangroves\", \"Moss and Lichen\"\n",
    "]\n",
    "\n",
    "df[landcover_cols] = (\n",
    "    df[landcover_cols]\n",
    "        .apply(pd.to_numeric, errors=\"coerce\")\n",
    "        .fillna(0)\n",
    "        .astype(\"float32\")\n",
    ")\n",
    "print(f\"Converted landcover columns to float32., calculating shannon entropy...\")\n",
    "if \"shannon_entropy\" not in df.columns:\n",
    "    df[\"shannon_entropy\"] = df[landcover_cols].apply(\n",
    "        compute_shannon_entropy, axis=1)\n",
    "    print(\"Shannon entropy computed.\")\n",
    "\n",
    "\n",
    "df.to_parquet(\"D:/NestEO_test_outputs/grid_1200m/filtered_lc_props_all_zones_107million.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d9a06c",
   "metadata": {},
   "source": [
    "#  The subset selection on the pre-filtered 1200m landcover proportions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d52713",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_parquet(\"D:/NestEO_test_outputs/grid_1200m/filtered_lc_props_all_zones_107million.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc691c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17e23a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_parquet(\"D:/NestEO_test_outputs/grid_1200m/grid_1200m_subsets_after_10k.parquet\")\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1e5540",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_250k = pd.read_parquet(\"D:/NestEO_test_outputs/grid_1200m/grid_1200m_subsets_after_250k.parquet\")\n",
    "df_250k.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834bf770",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_10k = df_250k[df_250k[\"subset_10k\"]!='none']\n",
    "df_25k = df_250k[df_250k[\"subset_25k\"]!='none']\n",
    "df_50k = df_250k[df_250k[\"subset_50k\"]!='none']\n",
    "df_100k = df_250k[df_250k[\"subset_100k\"]!='none']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db43cbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Ensure subset columns exist\n",
    "for col in [f\"subset_10k\", \"subset_25k\",  \"subset_50k\", \"subset_100k\", \"subset_250k\", \"subset_phase\"]:\n",
    "    if col not in df.columns:\n",
    "        df[col] = \"none\"\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6a9055",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Step 4: Map back subset_10k and subset_phase to full df\n",
    "df.loc[df_10k.index, \"subset_10k\"] = \"10k\"\n",
    "df.loc[df_25k.index, \"subset_25k\"] = \"25k\"\n",
    "df.loc[df_50k.index, \"subset_50k\"] = \"50k\"\n",
    "df.loc[df_100k.index, \"subset_100k\"] = \"100k\"\n",
    "df.loc[df_250k.index, \"subset_250k\"] = \"250k\"\n",
    "df.loc[df_250k.index, \"subset_phase\"] = df_250k[\"subset_phase\"]\n",
    "\n",
    "print(f\"Subset 10k rows correctly mapped back: {(df['subset_10k'] == '10k').sum():,} rows.\")\n",
    "print(f\"Subset 10k rows correctly mapped back: {(df['subset_25k'] == '25k').sum():,} rows.\")\n",
    "print(f\"Subset 10k rows correctly mapped back: {(df['subset_50k'] == '50k').sum():,} rows.\")\n",
    "print(f\"Subset 10k rows correctly mapped back: {(df['subset_100k'] == '100k').sum():,} rows.\")\n",
    "print(f\"Subset 10k rows correctly mapped back: {(df['subset_250k'] == '250k').sum():,} rows.\")\n",
    "\n",
    "# Now you can run your optimized select_hybrid_subsets() safely starting from 25k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e295d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Choose a previously created subsets if available\n",
    "\n",
    "# import pandas as pd\n",
    "# df = pd.read_parquet(\"D:/NestEO_test_outputs/grid_1200m/lc_1200m_subsets_after_100k.parquet\")\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba0f0d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04819cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Newest Batch sizes and restarts and save each level, plus batch wise update not individual tiles, Pick the purest in phase 1\n",
    "\n",
    "from scipy.stats import entropy\n",
    "import numpy as np, pandas as pd\n",
    "rng = np.random.default_rng(42)                               # reproducible\n",
    "\n",
    "def compute_shannon_entropy(row):\n",
    "    p = np.asarray(row, dtype=\"float32\")\n",
    "    p = p[p > 0]\n",
    "    return entropy(p, base=2)\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "def select_hybrid_subsets(\n",
    "    df,\n",
    "    landcover_cols,\n",
    "    targets,                    # dict: label -> 1-d numpy (sum=1)\n",
    "    sizes,                      # dict: label -> int (# rows requested)\n",
    "    class_thresholds,\n",
    "    phase_proportions=[0.4, 0.1, 0.4],      # must sum to 0.9\n",
    "    batch_size=25,             # int or dict\n",
    "    sample_pool_size=2500,     # int or dict\n",
    "    save_path_template=None    # e.g. \"out_with_{label}.parquet\"\n",
    "):\n",
    "    assert abs(sum(phase_proportions) - 0.9) < 1e-6, \\\n",
    "        \"phase_proportions must sum to 0.9 (10 % is Phase-4 / manual).\"\n",
    "\n",
    "    df = df.copy()\n",
    "    print(f\"Initial size: {len(df):,} rows\")\n",
    "    df[landcover_cols] = (\n",
    "        df[landcover_cols]\n",
    "          .apply(pd.to_numeric, errors=\"coerce\")\n",
    "          .fillna(0)\n",
    "          .astype(\"float32\")\n",
    "    )\n",
    "    print(f\"Converted landcover columns to float32., calculating shannon entropy...\")\n",
    "    if \"shannon_entropy\" not in df.columns:\n",
    "        df[\"shannon_entropy\"] = df[landcover_cols].apply(\n",
    "            compute_shannon_entropy, axis=1)\n",
    "        print(\"Shannon entropy computed.\")\n",
    "\n",
    "    if \"subset_phase\" not in df.columns:\n",
    "        df[\"subset_phase\"] = \"none\"\n",
    "\n",
    "    # -----------------------------------------------------------------\n",
    "    for label, size in sizes.items():\n",
    "        print(f\"\\n=== Selecting subset {label} (request {size:,} rows) ===\")\n",
    "        subset_col = f\"subset_{label}\"\n",
    "        if subset_col not in df.columns:\n",
    "            df[subset_col] = \"none\"\n",
    "\n",
    "        present = (df[subset_col] != \"none\").sum()\n",
    "        if present >= int(size * 0.9)-1:\n",
    "            print(f\"[{label}] already ≥ 90 % filled ({present:,}/{size:,}). \"\n",
    "                  \"Skipping selection and re-using those rows.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n=== Building subset {label}  (need {size:,}) ===\")\n",
    "\n",
    "        # ---------- per-subset batch / pool sizes --------------------\n",
    "        this_batch = batch_size[label]     if isinstance(batch_size, dict)     else batch_size\n",
    "        this_pool  = sample_pool_size[label] if isinstance(sample_pool_size, dict) else sample_pool_size\n",
    "\n",
    "        # ---------- rows reused from earlier subset levels ----------\n",
    "        earlier = [k for k in sizes if int(k.replace(\"k\", \"\")) < int(label.replace(\"k\", \"\"))]\n",
    "        reused = set()\n",
    "        for k in earlier:\n",
    "            reused.update(df.index[df[f\"subset_{k}\"] != \"none\"])\n",
    "\n",
    "        df.loc[list(reused), subset_col]      = label\n",
    "        df.loc[list(reused), \"subset_phase\"]  = f\"{label}_reuse\"\n",
    "\n",
    "        auto_quota = int(size * 0.9) - len(reused)\n",
    "        if auto_quota <= 0:\n",
    "            print(f\"[{label}] quota satisfied by reuse ({len(reused):,}).\")\n",
    "            if save_path_template:\n",
    "                df.to_parquet(save_path_template.format(label=label))\n",
    "            continue\n",
    "\n",
    "        p1 = int(auto_quota * phase_proportions[0])\n",
    "        p2 = int(auto_quota * phase_proportions[1])\n",
    "        p3 = auto_quota - p1 - p2\n",
    "\n",
    "        selected = set(reused)\n",
    "        phase_tag = pd.Series(\"none\", index=df.index, dtype=\"object\")\n",
    "\n",
    "        # ---------------- Phase 1  (dominant) ------------------------\n",
    "        p1_idx = []\n",
    "        for i, lc in enumerate(landcover_cols):\n",
    "            need = int(p1 * targets[label][i])\n",
    "            if need == 0: continue\n",
    "            cand = df[(df[lc] >= class_thresholds[i]) & (~df.index.isin(selected))]\n",
    "            if cand.empty: continue\n",
    "            # pick = cand.sample(\n",
    "            #     n=min(need, len(cand)),\n",
    "            #     random_state=rng.integers(1e9)).index.tolist()\n",
    "            \n",
    "            pick = cand.sort_values(by=lc, ascending=False).head(min(need, len(cand))).index.tolist()\n",
    "\n",
    "            p1_idx.extend(pick)\n",
    "        short = p1 - len(p1_idx)\n",
    "        if short > 0:\n",
    "            extra = df.loc[\n",
    "                (~df.index.isin(selected)) &\n",
    "                (df[landcover_cols].max(axis=1) >= 0.5)\n",
    "            ].sample(n=short, random_state=rng.integers(1e9)).index.tolist()\n",
    "            p1_idx.extend(extra)\n",
    "\n",
    "        df.loc[p1_idx, subset_col] = label\n",
    "        phase_tag.loc[p1_idx] = f\"{label}_phase1\"\n",
    "        selected.update(p1_idx)\n",
    "        print(f\"Phase-1 picked {len(p1_idx):,}\")\n",
    "\n",
    "        # ---------------- Phase 2  (entropy) -------------------------\n",
    "\n",
    "        ent_pool = df.loc[~df.index.isin(selected)]\n",
    "\n",
    "        # SAMPLE a smaller number of candidates to sort\n",
    "        if len(ent_pool) > 6 * p2:   # if more than 5× needed\n",
    "            ent_pool = ent_pool.sample(n=6*p2, random_state=rng.integers(1e9))\n",
    "\n",
    "        p2_idx = (ent_pool.sort_values(\"shannon_entropy\", ascending=False)\n",
    "                        .head(min(p2, len(ent_pool)))\n",
    "                        .index.tolist())\n",
    "\n",
    "        # p2_idx = ent_pool.nlargest(p2, columns=\"shannon_entropy\").index.tolist()\n",
    "\n",
    "        # p2_idx = ent_pool.nlargest(p2, columns=\"shannon_entropy\").index.tolist()\n",
    "\n",
    "        # ent_pool = df.loc[~df.index.isin(selected)]\n",
    "        # p2_idx = (ent_pool.sort_values(\"shannon_entropy\", ascending=False)\n",
    "        #                   .head(min(p2, len(ent_pool))).index.tolist())\n",
    "        df.loc[p2_idx, subset_col] = label\n",
    "        phase_tag.loc[p2_idx] = f\"{label}_phase2\"\n",
    "        selected.update(p2_idx)\n",
    "        print(f\"Phase-2 picked {len(p2_idx):,}\")\n",
    "\n",
    "        # ---------------- Phase 3  (greedy match, batch-wise) -------------------\n",
    "\n",
    "        p3_idx = []\n",
    "        cum = df.loc[list(selected), landcover_cols].sum().values\n",
    "        n_sel = len(selected)\n",
    "\n",
    "        remain = df.loc[~df.index.isin(selected)]\n",
    "        from tqdm import tqdm\n",
    "        steps = (p3 + this_batch - 1) // this_batch\n",
    "        pbar  = tqdm(total=steps, desc=f\"{label} phase-3\", leave=False)\n",
    "\n",
    "        while len(p3_idx) < p3 and not remain.empty:\n",
    "            remain = df.loc[~df.index.isin(selected)]\n",
    "            if remain.empty:\n",
    "                print(f\"Phase-3: no more candidates available.\")\n",
    "                break\n",
    "\n",
    "            from joblib import Parallel, delayed\n",
    "\n",
    "            # inside your phase 3 loop...\n",
    "\n",
    "            # sample and batchify\n",
    "            pool = remain.sample(\n",
    "                n=min(this_pool, len(remain)),\n",
    "                random_state=rng.integers(1e9)\n",
    "            )\n",
    "            pool_X = pool[landcover_cols].values\n",
    "            pool_idx = pool.index.to_numpy()\n",
    "\n",
    "            n_batches = len(pool_idx) // this_batch\n",
    "            if n_batches == 0:\n",
    "                print(\"Warning: not enough candidates to form even one batch.\")\n",
    "                break\n",
    "\n",
    "            # split pool into batches\n",
    "            pool_X_batches = pool_X[:n_batches * this_batch].reshape(n_batches, this_batch, -1)\n",
    "\n",
    "            def compute_error(i):\n",
    "                batch_sum = pool_X_batches[i].sum(axis=0)\n",
    "                error = np.mean(( (cum + batch_sum) / (n_sel + this_batch) - targets[label]) ** 2)\n",
    "                return i, error\n",
    "\n",
    "            # parallel compute errors\n",
    "            batch_errors = Parallel(n_jobs=-1, backend=\"threading\")(\n",
    "                delayed(compute_error)(i) for i in range(n_batches)\n",
    "            )\n",
    "\n",
    "            batch_errors = sorted(batch_errors, key=lambda x: x[1])\n",
    "            best_batch_idx = batch_errors[0][0]\n",
    "            best_idxs = pool_idx[best_batch_idx*this_batch : (best_batch_idx+1)*this_batch]\n",
    "\n",
    "            # update\n",
    "            p3_idx.extend(best_idxs.tolist())\n",
    "            selected.update(best_idxs.tolist())\n",
    "            cum += pool_X_batches[best_batch_idx].sum(axis=0)\n",
    "            n_sel += len(best_idxs)\n",
    "\n",
    "            # clean\n",
    "            del pool, pool_X, pool_idx, pool_X_batches, batch_errors\n",
    "            import gc; gc.collect()\n",
    "\n",
    "\n",
    "            pbar.update(1)\n",
    "            if len(p3_idx) >= p3:\n",
    "                break\n",
    "\n",
    "        pbar.close()\n",
    "\n",
    "        df.loc[p3_idx, subset_col] = label\n",
    "        phase_tag.loc[p3_idx]      = f\"{label}_phase3\"\n",
    "        print(f\"Phase-3 picked {len(p3_idx):,}\")\n",
    "\n",
    "        # ---------- commit phase labels & save progress -------------\n",
    "        df.loc[phase_tag != \"none\", \"subset_phase\"] = phase_tag[phase_tag != \"none\"]\n",
    "\n",
    "        done = (df[subset_col] != \"none\").sum()\n",
    "        print(f\"Finished {label}: {done:,}/{size:,} rows labeled \"\n",
    "              f\"(auto {len(selected) - len(reused):,}, reuse {len(reused):,}).\")\n",
    "\n",
    "        if save_path_template:\n",
    "            # save only picked rows, not all\n",
    "            df.loc[df[subset_col] == label].to_parquet(save_path_template.format(label=label))\n",
    "            if label in [\"500k\", \"1000K\", \"1500K\"]:\n",
    "                # save all rows for these labels\n",
    "                # df.loc[df[subset_col] == label].to_parquet(save_path_template.format(label=label))\n",
    "                out_path = save_path_template.format(label=label)\n",
    "                df.to_parquet(out_path)\n",
    "                print(f\"Saved progress → {out_path}\")\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21a3eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1.  Load the CSV that contains your level‐wise proportions\n",
    "# ------------------------------------------------------------------\n",
    "csv_path = \"D:/NestEO_test_outputs/tiered_props_all_levels.csv\"\n",
    "tbl = pd.read_csv(csv_path)\n",
    "tbl.rename(columns={tbl.columns[0]: \"Land Cover\"}, inplace=True)\n",
    "ordered = [\n",
    "    \"Open Seas\", \"Tree Cover\", \"Shrubland\", \"Grassland\", \"Cropland\",\n",
    "    \"Built-up\", \"Bare/Sparse Vegetation\", \"Snow and Ice\",\n",
    "    \"Permanent Water Bodies\", \"Herbaceous Wetland\",\n",
    "    \"Mangroves\", \"Moss and Lichen\"\n",
    "]\n",
    "\n",
    "base_1200m = (\n",
    "    tbl.set_index(\"Land Cover\")          # align by name\n",
    "    #    .loc[ordered, \"Global 1200m\"]     # % values\n",
    "        .loc[ordered, \"Medium (1200m)\"] \n",
    "       .values.astype(\"float32\") / 100.0 # to fractions\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1.  importance weights  (edit if you wish)\n",
    "# ------------------------------------------------------------\n",
    "weights = np.array([\n",
    "    0.25, 1.0, 0.9, 1.1, 1.5,\n",
    "    2.0, 0.9, 1.1, \n",
    "    1.2, 1.5, \n",
    "    2.0, 1.5\n",
    "], dtype=\"float32\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2.  bias factors  –  small batch = strong oversampling\n",
    "# ------------------------------------------------------------\n",
    "bias = {\n",
    "    \"10k\":   1.5,\n",
    "    \"25k\":   1.4,\n",
    "    \"50k\":   1.3,\n",
    "    \"100k\":  1.2,\n",
    "    \"250k\":  1.1,\n",
    "    \"500k\":  1.0,\n",
    "    \"750k\":  0.9,\n",
    "    \"1000k\": 0.8,\n",
    "    \"1500k\": 0.7,\n",
    "}\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 3.  availability ceiling  (fraction of majority-class tiles)\n",
    "#     → use real numbers when you have them\n",
    "# ------------------------------------------------------------\n",
    "avail = np.minimum(base_1200m, 1.0).astype(\"float32\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4.  progressive targets  (now bias really matters)\n",
    "#     formula:  blend = (1-μ)*base + μ*biased ,\n",
    "#               where μ = (b-1)/(max_bias-1)\n",
    "# ------------------------------------------------------------\n",
    "max_bias = max(bias.values())  # = 1.6 here\n",
    "target_distributions = {}\n",
    "\n",
    "for name, b in bias.items():\n",
    "    mu   = (b - 1.0) / (max_bias - 1.0)   # 0 … 1\n",
    "    biased = weights * base_1200m\n",
    "    raw    = (1 - mu) * base_1200m + mu * biased\n",
    "    raw    = np.minimum(raw, avail)       # respect ceiling\n",
    "    target = raw / raw.sum()              # normalise to 1\n",
    "    target_distributions[name] = target\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 5.  verify each row differs\n",
    "# ------------------------------------------------------------------\n",
    "pd.set_option(\"display.precision\", 2)\n",
    "print(pd.DataFrame(target_distributions, index=ordered).T * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081933e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff96a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# 7.  Subset-size dictionary\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "# Subset sizes\n",
    "subset_sizes = {\n",
    "    \"10k\": 10_000,\n",
    "    \"25k\": 25_000,\n",
    "    \"50k\": 50_000,\n",
    "    \"100k\": 100_000,\n",
    "    \"250k\": 250_000,\n",
    "    \"500k\": 500_000,\n",
    "    \"750k\": 750_000,\n",
    "    \"1000k\": 1_000_000,\n",
    "    \"1500k\": 1_500_000\n",
    "}\n",
    "\n",
    "pure_class_thresholds = np.array([\n",
    "    0.80,  # Open Seas\n",
    "    0.70,  # Tree Cover\n",
    "    0.70,  # Shrubland\n",
    "    0.65,  # Grassland\n",
    "    0.60,  # Cropland\n",
    "    0.20,  # Built-up\n",
    "    0.60,  # Bare / Sparse Vegetation\n",
    "    0.40,  # Snow and Ice\n",
    "    0.50,  # Permanent Water Bodies\n",
    "    0.40,  # Herbaceous Wetland\n",
    "    0.05,  # Mangroves\n",
    "    0.10   # Moss and Lichen\n",
    "])\n",
    "\n",
    "# Usage:\n",
    "phase_proportions = [0.4, 0.1, 0.4]  # 50% pure, 10% high-entropy, 40% target matching 10% left for manual based on dataset availability\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f05450",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sizes = { \"10k\": 20, \"25k\": 25, \"50k\": 50,\n",
    "                \"100k\": 75, \"250k\": 150, \"500k\": 150,\n",
    "                \"750k\": 375, \"1000k\": 500, \"1500k\": 750 }\n",
    "\n",
    "pool_sizes  = { \"10k\": 2500, \"25k\": 2500, \"50k\": 3000,\n",
    "                \"100k\": 3500, \"250k\": 3500, \"500k\": 3500,\n",
    "                \"750k\": 3750, \"1000k\": 5000, \"1500k\": 5000 }  #{ k: v*50 for k, v in batch_sizes.items() }  # example\n",
    "\n",
    "df = select_hybrid_subsets(\n",
    "        df,\n",
    "        ordered,\n",
    "        targets=target_distributions,\n",
    "        sizes=subset_sizes,\n",
    "        class_thresholds=pure_class_thresholds,\n",
    "        batch_size=batch_sizes,\n",
    "        sample_pool_size=pool_sizes,\n",
    "        save_path_template=\"D:/NestEO_test_outputs/grid_1200m/grid_1200m_subsets_after_{label}.parquet\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b5e484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 250k samples selected for 1200m levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbc1393",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d757f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f03bb6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "63a0ad7f",
   "metadata": {},
   "source": [
    "# Previous code using hardcoded for each subset level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e258cc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "landcover_columns = [\n",
    "    \"Open Seas\", \"Tree Cover\", \"Shrubland\", \"Grassland\", \"Cropland\", \"Built-up\",\n",
    "    \"Bare / Sparse Vegetation\", \"Snow and Ice\", \"Permanent Water Bodies\",\n",
    "    \"Herbaceous Wetland\", \"Mangroves\", \"Moss and Lichen\"\n",
    "]\n",
    "\n",
    "target_1200m = np.array([\n",
    "    2.0,  # Open Seas\n",
    "    22.0, # Tree Cover\n",
    "    7.0,  # Shrubland\n",
    "    12.0, # Grassland\n",
    "    18.0, # Cropland\n",
    "    10.0,  # Built-up\n",
    "    7.0, # Bare / Sparse Vegetation\n",
    "    4.0,  # Snow and Ice\n",
    "    7.0, # Permanent Water Bodies\n",
    "    5.0,  # Herbaceous Wetland\n",
    "    3.0,  # Mangroves\n",
    "    3.0   # Moss and Lichen\n",
    "]) / 100\n",
    "# -----------------------------------------------------------------------------------\n",
    "target_10k = np.array([\n",
    "    0.5,  # Open Seas\n",
    "    16.0, # Tree Cover\n",
    "    6.0,  # Shrubland\n",
    "    13.0, # Grassland\n",
    "    15.0, # Cropland\n",
    "    8.0,  # Built-up\n",
    "    10.0, # Bare / Sparse Vegetation\n",
    "    4.0,  # Snow and Ice\n",
    "    10.0, # Permanent Water Bodies\n",
    "    6.0,  # Herbaceous Wetland\n",
    "    3.0,  # Mangroves\n",
    "    8.5   # Moss and Lichen\n",
    "]) / 100\n",
    "\n",
    "target_25k = np.array([\n",
    "    1.0,  # Open Seas\n",
    "    18.0, # Tree Cover\n",
    "    6.0,  # Shrubland\n",
    "    14.0, # Grassland\n",
    "    15.0, # Cropland\n",
    "    7.0,  # Built-up\n",
    "    11.0, # Bare / Sparse Vegetation\n",
    "    4.0,  # Snow and Ice\n",
    "    10.0, # Permanent Water Bodies\n",
    "    6.0,  # Herbaceous Wetland\n",
    "    2.5,  # Mangroves\n",
    "    5.5   # Moss and Lichen\n",
    "]) / 100\n",
    "\n",
    "target_50k = np.array([\n",
    "    1.2,  # Open Seas\n",
    "    20.0, # Tree Cover\n",
    "    6.5,  # Shrubland\n",
    "    15.0, # Grassland\n",
    "    14.5, # Cropland\n",
    "    6.0,  # Built-up\n",
    "    12.0, # Bare / Sparse Vegetation\n",
    "    4.0,  # Snow and Ice\n",
    "    9.5, # Permanent Water Bodies\n",
    "    5.5,  # Herbaceous Wetland\n",
    "    2.0,  # Mangroves\n",
    "    3.8   # Moss and Lichen\n",
    "]) / 100\n",
    "\n",
    "target_100k = np.array([\n",
    "    1.5,  # Open Seas\n",
    "    22.0, # Tree Cover\n",
    "    7.0,  # Shrubland\n",
    "    16.0, # Grassland\n",
    "    13.5, # Cropland\n",
    "    5.0,  # Built-up\n",
    "    12.0, # Bare / Sparse Vegetation\n",
    "    4.0,  # Snow and Ice\n",
    "    10.0, # Permanent Water Bodies\n",
    "    5.0,  # Herbaceous Wetland\n",
    "    1.5,  # Mangroves\n",
    "    2.5   # Moss and Lichen\n",
    "]) / 100\n",
    "\n",
    "target_250k = np.array([\n",
    "    1.8,  # Open Seas\n",
    "    24.0, # Tree Cover\n",
    "    7.5,  # Shrubland\n",
    "    17.0, # Grassland\n",
    "    12.5, # Cropland\n",
    "    4.0,  # Built-up\n",
    "    12, # Bare / Sparse Vegetation\n",
    "    4.0,  # Snow and Ice\n",
    "    11, # Permanent Water Bodies\n",
    "    4,  # Herbaceous Wetland\n",
    "    1.0,  # Mangroves\n",
    "    1.2   # Moss and Lichen\n",
    "]) / 100\n",
    "\n",
    "target_500k = np.array([\n",
    "    1.8,  # Open Seas\n",
    "    24.0, # Tree Cover\n",
    "    7.5,  # Shrubland\n",
    "    17.0, # Grassland\n",
    "    12.5, # Cropland\n",
    "    4.0,  # Built-up\n",
    "    12, # Bare / Sparse Vegetation\n",
    "    4.0,  # Snow and Ice\n",
    "    11, # Permanent Water Bodies\n",
    "    4,  # Herbaceous Wetland\n",
    "    1.0,  # Mangroves\n",
    "    1.2   # Moss and Lichen\n",
    "]) / 100\n",
    "\n",
    "target_750k = np.array([\n",
    "    1.8,  # Open Seas\n",
    "    24.0, # Tree Cover\n",
    "    7.5,  # Shrubland\n",
    "    17.0, # Grassland\n",
    "    12.5, # Cropland\n",
    "    4.0,  # Built-up\n",
    "    12, # Bare / Sparse Vegetation\n",
    "    4.0,  # Snow and Ice\n",
    "    11, # Permanent Water Bodies\n",
    "    4,  # Herbaceous Wetland\n",
    "    1.0,  # Mangroves\n",
    "    1.2   # Moss and Lichen\n",
    "]) / 100\n",
    "\n",
    "target_1000k = np.array([\n",
    "    1.8,  # Open Seas\n",
    "    24.0, # Tree Cover\n",
    "    7.5,  # Shrubland\n",
    "    17.0, # Grassland\n",
    "    12.5, # Cropland\n",
    "    4.0,  # Built-up\n",
    "    12, # Bare / Sparse Vegetation\n",
    "    4.0,  # Snow and Ice\n",
    "    11, # Permanent Water Bodies\n",
    "    4,  # Herbaceous Wetland\n",
    "    1.0,  # Mangroves\n",
    "    1.2   # Moss and Lichen\n",
    "]) / 100\n",
    "print(target_10k.sum())\n",
    "print(target_25k.sum())\n",
    "print(target_50k.sum())\n",
    "print(target_100k.sum())\n",
    "print(target_250k.sum())\n",
    "print(target_500k.sum())\n",
    "print(target_750k.sum())    \n",
    "print(target_1000k.sum())       \n",
    "                \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b32e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "target_distributions = {\n",
    "    \"10k\": target_10k,\n",
    "    \"25k\":target_25k,\n",
    "    \"50k\": target_50k,    \n",
    "    \"100k\": target_100k,\n",
    "    \"250k\":target_250k,\n",
    "    \"500k\": target_500k,\n",
    "    \"750k\": target_750k,\n",
    "    \"1000k\": target_1000k\n",
    "}\n",
    "\n",
    "# Subset sizes\n",
    "subset_sizes = {\n",
    "    \"10k\": 10_000,\n",
    "    \"25k\": 25_000,\n",
    "    \"50k\": 50_000,\n",
    "    \"100k\": 100_000,\n",
    "    \"250k\": 250_000,\n",
    "    \"500k\": 500_000,\n",
    "    \"750k\": 750_000,\n",
    "    \"1000k\": 1_000_000\n",
    "}\n",
    "\n",
    "pure_class_thresholds = np.array([\n",
    "    0.80,  # Open Seas\n",
    "    0.70,  # Tree Cover\n",
    "    0.70,  # Shrubland\n",
    "    0.65,  # Grassland\n",
    "    0.60,  # Cropland\n",
    "    0.20,  # Built-up\n",
    "    0.60,  # Bare / Sparse Vegetation\n",
    "    0.40,  # Snow and Ice\n",
    "    0.50,  # Permanent Water Bodies\n",
    "    0.40,  # Herbaceous Wetland\n",
    "    0.10,  # Mangroves\n",
    "    0.10   # Moss and Lichen\n",
    "])\n",
    "\n",
    "# Usage:\n",
    "phase_proportions = [0.4, 0.1, 0.4]  # 50% pure, 10% high-entropy, 40% target matching\n",
    "\n",
    "df_labeled = select_hybrid_subsets(\n",
    "    df,\n",
    "    landcover_cols=landcover_columns,\n",
    "    targets=target_distributions,\n",
    "    sizes=subset_sizes,\n",
    "    class_thresholds=pure_class_thresholds,\n",
    "    phase_proportions=phase_proportions,\n",
    "    batch_size=50,\n",
    "    sample_pool_size=2500\n",
    ")\n",
    "\n",
    "df_labeled.to_parquet(\"D:/MajorTOMExpand/0. DatasetsFolder/13.Major-TOM/MajorTOM_metadata_with_computed_attributes_subsets.parquet\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a95d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labeled2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac439acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Example loading from CSV (you'd replace this with your real DataFrame)\n",
    "# df = pd.read_csv('your_file.csv')\n",
    "df = df_labeled2.copy()\n",
    "\n",
    "# Assuming your DataFrame is already loaded and named `df`\n",
    "# The goal is to propagate subset labels upward\n",
    "\n",
    "subset_cols = ['subset_100k', 'subset_250k', 'subset_500k', 'subset_750k', 'subset_1000k']\n",
    "\n",
    "# Process each column starting from the second, copying non-'none' values from lower levels\n",
    "for i in range(1, len(subset_cols)):\n",
    "    current_col = subset_cols[i]\n",
    "    lower_cols = subset_cols[:i]\n",
    "    \n",
    "    # Create a mask of rows where the current column is 'none'\n",
    "    mask = df[current_col] == 'none'\n",
    "    \n",
    "    # For those rows, check the first non-'none' value from lower levels\n",
    "    for col in reversed(lower_cols):\n",
    "        df.loc[mask & (df[col] != 'none'), current_col] = df[col]\n",
    "\n",
    "# Now the higher subset columns (like 500k, 750k, etc.) also include inherited lower levels\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8516e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet(\"D:/MajorTOMExpand/0. DatasetsFolder/13.Major-TOM/MajorTOM_metadata_with_computed_attributes_subsetsMentioned.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9b8f46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "43815015",
   "metadata": {},
   "source": [
    "# Custom 200k subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfefa86",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_grid_cells = pd.read_csv(\"D:/2020_subset_grid.csv\")\n",
    "filter_grid_cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92da228",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge = df.merge(filter_grid_cells, on='grid_cell', how='inner')\n",
    "df_merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01ac85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge_export = df_merge[[\"grid_cell\", \"subset_phase\", \"subset_100k\", \"subset_250k\", \"subset_500k\", \"subset_750k\", \"subset_1000k\"]]\n",
    "df_merge_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55470d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge_export.to_csv(\"D:/MajorTOMExpand/0. DatasetsFolder/13.Major-TOM/MajorTOM_metadata_with_computed_cusotmSubsetsGridCells.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86fdacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge_dedup.to_parquet(\"D:/MajorTOMExpand/0. DatasetsFolder/13.Major-TOM/MajorTOM_metadata_with_computed_attributes_2020_subset_grid.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d2e09b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa10e9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a7fb63",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PytKerasNewHQ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
